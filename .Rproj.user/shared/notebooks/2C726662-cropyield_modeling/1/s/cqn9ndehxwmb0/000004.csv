"0","# predicting YIELD"
"0","drybean.sub <- drybean %>%"
"0","  select(kgha, ends_with(""_mean"")) %>%"
"0","  na.omit()"
"0",""
"0",""
"0",""
"0",""
"0","# where are the coefficients NA. We will remove these from the model"
"0","drybean.reduced <- drybean.sub %>% select(-c(X9_mean, X18_mean, X19_mean, X27_mean,"
"0","                                            X28_mean,  X29_mean, X36_mean,  X37_mean, X38_mean,"
"0","                                            X39_mean, X45_mean,  X46_mean, X47_mean,  X48_mean,"
"0","                                            X49_mean,  X54_mean, X55_mean,  X56_mean, X57_mean,"
"0","                                            X58_mean, X59_mean,  X63_mean, X64_mean,  X65_mean, "
"0","                                            X66_mean,  X67_mean, X68_mean,  X69_mean, X72_mean, "
"0","                                            X73_mean, X74_mean,  X75_mean, X76_mean,  X77_mean,"
"0","                                            X78_mean,  X79_mean, X81_mean, X82_mean,  X83_mean,"
"0","                                            X84_mean, X85_mean, X86_mean,  X87_mean, X88_mean,"
"0","                                            X89_mean))"
"0",""
"0","# # Split the data into training and test set"
"0","set.seed(123)"
"0","training.samples <- drybean.reduced$kgha %>%"
"0","  createDataPartition(p = 0.8, list = FALSE)"
"0","train.data  <- drybean.reduced[training.samples, ]"
"0","test.data <- drybean.reduced[-training.samples, ]"
"0",""
"0",""
"0","# create a multiple regression model using all predictors"
"0","lm.fit <- lm(kgha ~ ., data = train.data)"
"0","summary(lm.fit)"
"1","
Call:
"
"1",""
"1","lm(formula = kgha ~ ., data = train.data)"
"1",""
"1","

"
"1",""
"1","Residuals:
"
"1","   Min "
"1","    1Q "
"1","Median "
"1","    3Q "
"1","   Max "
"1","
"
"1","-964.1 "
"1","-143.5 "
"1","  -0.6 "
"1"," 160.4 "
"1"," 646.8 "
"1","
"
"1","
Coefficients:
"
"1","           "
"1"," Estimate"
"1"," Std. Error"
"1"," t value"
"1"," Pr(>|t|)"
"1","  "
"1","
(Intercept)"
"1","     2642"
"1","       1142"
"1","   2.312"
"1","   0.0250"
"1"," *"
"1","
X0_mean    "
"1"," -1331243"
"1","     814824"
"1","  -1.634"
"1","   0.1087"
"1","  "
"1","
X1_mean    "
"1","  2386110"
"1","    3194943"
"1","   0.747"
"1","   0.4587"
"1","  "
"1","
X2_mean    "
"1"," -2078468"
"1","    3577648"
"1","  -0.581"
"1","   0.5639"
"1","  "
"1","
X3_mean    "
"1","   120035"
"1","     999554"
"1","   0.120"
"1","   0.9049"
"1","  "
"1","
X4_mean    "
"1","  1095182"
"1","     949443"
"1","   1.153"
"1","   0.2543"
"1","  "
"1","
X5_mean    "
"1"," -2675778"
"1","    3297591"
"1","  -0.811"
"1","   0.4210"
"1","  "
"1","
X6_mean    "
"1"," -4435222"
"1","    2592291"
"1","  -1.711"
"1","   0.0934"
"1"," ."
"1","
X7_mean    "
"1","  1779366"
"1","    1165548"
"1","   1.527"
"1","   0.1333"
"1","  "
"1","
X8_mean    "
"1","  5373635"
"1","    3626697"
"1","   1.482"
"1","   0.1448"
"1","  "
"1","
X10_mean   "
"1"," -1660153"
"1","    3043775"
"1","  -0.545"
"1","   0.5879"
"1","  "
"1","
X11_mean   "
"1","  2018519"
"1","    3249952"
"1","   0.621"
"1","   0.5374"
"1","  "
"1","
X12_mean   "
"1","  -400806"
"1","     923041"
"1","  -0.434"
"1","   0.6660"
"1","  "
"1","
X13_mean   "
"1"," -1512930"
"1","    1316345"
"1","  -1.149"
"1","   0.2560"
"1","  "
"1","
X14_mean   "
"1","  1492460"
"1","    2888840"
"1","   0.517"
"1","   0.6077"
"1","  "
"1","
X15_mean   "
"1"," -3433711"
"1","    2247103"
"1","  -1.528"
"1","   0.1329"
"1","  "
"1","
X16_mean   "
"1","  1897058"
"1","    1289745"
"1","   1.471"
"1","   0.1477"
"1","  "
"1","
X17_mean   "
"1","   179015"
"1","    3115759"
"1","   0.057"
"1","   0.9544"
"1","  "
"1","
X20_mean   "
"1"," -9577158"
"1","    9334788"
"1","  -1.026"
"1","   0.3099"
"1","  "
"1","
X21_mean   "
"1","  1363448"
"1","    7317764"
"1","   0.186"
"1","   0.8530"
"1","  "
"1","
X22_mean   "
"1","  1268159"
"1","   15290235"
"1","   0.083"
"1","   0.9342"
"1","  "
"1","
X23_mean   "
"1","  -816647"
"1","    5871003"
"1","  -0.139"
"1","   0.8899"
"1","  "
"1","
X24_mean   "
"1","  8329807"
"1","    5898761"
"1","   1.412"
"1","   0.1642"
"1","  "
"1","
X25_mean   "
"1","  3122562"
"1","    4576069"
"1","   0.682"
"1","   0.4982"
"1","  "
"1","
X26_mean   "
"1"," -1203873"
"1","    6062508"
"1","  -0.199"
"1","   0.8434"
"1","  "
"1","
X30_mean   "
"1","   629957"
"1","    8356684"
"1","   0.075"
"1","   0.9402"
"1","  "
"1","
X31_mean   "
"1"," -2563666"
"1","   15734028"
"1","  -0.163"
"1","   0.8712"
"1","  "
"1","
X32_mean   "
"1"," -4658682"
"1","    7068678"
"1","  -0.659"
"1","   0.5129"
"1","  "
"1","
X33_mean   "
"1"," -5483853"
"1","    6203708"
"1","  -0.884"
"1","   0.3810"
"1","  "
"1","
X34_mean   "
"1"," -4955095"
"1","    5136129"
"1","  -0.965"
"1","   0.3394"
"1","  "
"1","
X35_mean   "
"1","  5265528"
"1","    7405717"
"1","   0.711"
"1","   0.4804"
"1","  "
"1","
X40_mean   "
"1","  2764796"
"1","    2480929"
"1","   1.114"
"1","   0.2705"
"1","  "
"1","
X41_mean   "
"1","  2260522"
"1","    2319538"
"1","   0.975"
"1","   0.3346"
"1","  "
"1","
X42_mean   "
"1","  1522409"
"1","    2111482"
"1","   0.721"
"1","   0.4743"
"1","  "
"1","
X43_mean   "
"1"," -2328690"
"1","    2014857"
"1","  -1.156"
"1","   0.2534"
"1","  "
"1","
X44_mean   "
"1"," -2346326"
"1","    2261685"
"1","  -1.037"
"1","   0.3046"
"1","  "
"1","
X50_mean   "
"1","  1279433"
"1","    3172303"
"1","   0.403"
"1","   0.6885"
"1","  "
"1","
X51_mean   "
"1","  2787904"
"1","    2853295"
"1","   0.977"
"1","   0.3333"
"1","  "
"1","
X52_mean   "
"1","   102781"
"1","    2710721"
"1","   0.038"
"1","   0.9699"
"1","  "
"1","
X53_mean   "
"1"," -2939666"
"1","    2734663"
"1","  -1.075"
"1","   0.2877"
"1","  "
"1","
X60_mean   "
"1"," -9399504"
"1","    6331442"
"1","  -1.485"
"1","   0.1441"
"1","  "
"1","
X61_mean   "
"1","  3147672"
"1","    3088285"
"1","   1.019"
"1","   0.3131"
"1","  "
"1","
X62_mean   "
"1","  4693086"
"1","    3614450"
"1","   1.298"
"1","   0.2002"
"1","  "
"1","
X70_mean   "
"1","   292218"
"1","    2584073"
"1","   0.113"
"1","   0.9104"
"1","  "
"1","
X71_mean   "
"1"," -7811433"
"1","    5846909"
"1","  -1.336"
"1","   0.1877"
"1","  "
"1","
X80_mean   "
"1","  1702707"
"1","    3292147"
"1","   0.517"
"1","   0.6073"
"1","  "
"1","
"
"1","---
Signif. codes:  "
"1",""
"1","0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"
"1","
"
"1","
Residual standard error:"
"1"," "
"1","367.7"
"1"," "
"1","on"
"1"," "
"1","49"
"1"," "
"1","degrees of freedom"
"1","
"
"1","Multiple R-squared: "
"1"," "
"1","0.7831"
"1",",	Adjusted R-squared: "
"1"," "
"1","0.584"
"1"," "
"1","
F-statistic:"
"1"," "
"1","3.932"
"1"," "
"1","on"
"1"," "
"1","45"
"1"," "
"1","and"
"1"," "
"1","49"
"1"," "
"1","DF,  p-value:"
"1"," "
"1","2.756e-06"
"1","
"
"1","
"
"0","# predict using test data"
"0","predictions <- lm.fit %>% predict(test.data)"
"0",""
"0","# compute R MSE"
"0","RMSE <- RMSE(predictions, test.data$kgha)"
"0",""
"0",""
"0","# compute R-square"
"0","R2 <- R2(predictions, test.data$kgha)"
"0",""
"0","# return results"
"0","print(paste(""RMSE = "", RMSE))"
"1","[1]"
"1"," ""RMSE =  496.218800229783"""
"1","
"
"0","print(paste(""R-squared value is:"", R2))"
"1","[1]"
"1"," ""R-squared value is: 0.351117314743734"""
"1","
"
"0","# par(mfrow=c(2,2))"
"0","# plot(lm.fit)"
"0",""
"0","# check for normality "
"0","# library(fBasics)"
"0","# library(lmtest)"
"0",""
"0","# jarqueberaTest(lm.fit$residuals)"
"0","# dwtest(lm.fit)"
"0",""
"0","# run a linear model fit and see if there are some really collinear variables"
"0","# due to the nature of the hyperspectral indices, there are many colinear variables"
"0","# present in the dataset. lets remove some of those."
"0","# lm.fit <- lm(kgha ~ ., data = drybean.sub[,2:ncol(drybean.sub)])"
"0","# summary(lm.fit)"
"0",""
"0","# "
"0","# # #"
"0","# library(corrplot)"
"0","# x <-cor(train.data[, 2:ncol(train.data)])"
"0","# print(x)"
"0",""
"0","# # corrplot(x, type=""upper"", order=""hclust"")"
"0","# "
"0",""
"0",""
